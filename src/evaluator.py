import json
import os
import time
import base64
import pandas as pd
import concurrent.futures
from typing import List, Tuple, Dict, Any, Union


class ImageCaptionEvaluator:
    """
    Evaluates image captioning models by generating captions, evaluating similarity scores,
    measuring cost and latency, and aggregating results into a DataFrame.
    """

    def __init__(
        self,
        gen_models: List[Tuple[str, Any]],
        pricing: Dict[str, Dict[str, float]],
        judge_client: Any,
        image_dir: str
    ) -> None:
        """
        Initialize the evaluator with models, pricing info, judge client, and image directory.

        Args:
            gen_models: List of tuples (model_name, client_instance) for caption generation.
            pricing: Dictionary mapping model_name to input/output token pricing.
            judge_client: Client instance for judge LLM used to score captions.
            image_dir: Directory path containing images.
        """
        self.GEN_MODELS = gen_models
        self.PRICING = pricing
        self.judge_client = judge_client
        self.IMAGE_DIR = image_dir

    def describe_image(
        self,
        client: Any,
        model: str,
        image_path: str
    ) -> Tuple[str, int, int]:
        """
        Generate a caption for an image using the specified client and model.

        Args:
            client: Client instance to invoke the model.
            model: Model name or ID.
            image_path: Path to the image file.

        Returns:
            A tuple containing:
                - Generated caption (str)
                - Number of input tokens used (int)
                - Number of output tokens used (int)
            Returns error string and zeros on failure.
        """
        try:
            return self._generate_caption(client, model, image_path)
        except Exception as e:
            return f"[Error: {e}]", 0, 0

    def evaluate_caption(
        self,
        generated: str,
        reference: str
    ) -> int:
        """
        Use the judge LLM to score similarity between generated and reference captions.

        Args:
            generated: Caption generated by the model.
            reference: Ground truth caption.

        Returns:
            Integer similarity score between 0 and 10.
            Returns -1 if evaluation fails.
        """
        prompt = (
            f"You are an image caption evaluator.\n\n"
            f"Original caption:\n\"{reference}\"\n\n"
            f"Generated caption:\n\"{generated}\"\n\n"
            f"Rate similarity and accuracy on a scale of 0 (not accurate) to 10 (very accurate). Reply with a single number."
        )
        try:
            response = self.judge_client.chat.completions.create(
                model=self.judge_client.model,
                messages=[{"role": "user", "content": prompt}]
            )
            score_str = response.choices[0].message.content.strip()
            return int(score_str) if score_str.isdigit() else -1
        except Exception:
            return -1

    def process_single(
        self,
        image_filename: str,
        original_caption: str
    ) -> List[Dict[str, Union[str, int, float]]]:
        """
        Process one image-caption pair across all models: generate captions, evaluate,
        calculate cost and latency, and collect results.

        Args:
            image_filename: Filename of the image.
            original_caption: Ground truth caption for the image.

        Returns:
            List of dictionaries with evaluation results for each model.
            Empty list if image not found.
        """
        image_path = os.path.join(self.IMAGE_DIR, image_filename)
        if not os.path.exists(image_path):
            return []

        results = []
        for model_name, client in self.GEN_MODELS:
            start_time = time.time()
            gen_caption, in_tok, out_tok = self.describe_image(client, model_name, image_path)
            latency = round(time.time() - start_time, 4)
            cost = (in_tok / 1e6) * self.PRICING[model_name]["input"] + (out_tok / 1e6) * self.PRICING[model_name]["output"]
            score = self.evaluate_caption(gen_caption, original_caption)
            results.append({
                "image_filename": image_filename,
                "model": model_name,
                "generated_caption": gen_caption,
                "original_caption": original_caption,
                "similarity_score": score,
                "latency": latency,
                "cost_usd": round(cost, 6)
            })
        return results

    def evaluate_dataset(
        self,
        image_caption_pairs: List[Tuple[str, str]],
        max_workers: int = 4
    ) -> pd.DataFrame:
        """
        Evaluate a list of image-caption pairs concurrently.

        Args:
            image_caption_pairs: List of tuples (image_filename, ground truth caption).
            max_workers: Maximum number of concurrent threads.

        Returns:
            Pandas DataFrame containing evaluation results across all models and images.
        """
        results = []
        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
            futures = [executor.submit(self.process_single, path, caption) for path, caption in image_caption_pairs]
            for future in concurrent.futures.as_completed(futures):
                results.extend(future.result())
        return pd.DataFrame(results)

    def _generate_caption(
        self,
        client: Any,
        model_name: str,
        image_path: str
    ) -> Tuple[str, int, int]:
        """
        Internal helper to generate caption using either OpenAI-style or Bedrock-style client.

        Args:
            client: Client instance.
            model_name: Model identifier.
            image_path: Path to the image.

        Returns:
            Tuple of (caption string, input tokens, output tokens).
        """
        if hasattr(client, "chat"):  # OpenAI style client
            return self._generate_caption_openai(client, model_name, image_path)
        else:  # Bedrock style client
            return self._generate_caption_bedrock(client, model_name, image_path)

    def _generate_caption_openai(
        self,
        client: Any,
        model_name: str,
        image_path: str
    ) -> Tuple[str, int, int]:
        """
        Generate caption using OpenAI style client.

        Args:
            client: OpenAI client instance.
            model_name: Model identifier.
            image_path: Path to the image.

        Returns:
            Tuple of (caption string, input tokens, output tokens).
        """
        prompt = "Describe this image in one sentence."
        try:
            base64_image = self.encode_image_base64(image_path)
            messages = [
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": prompt},
                        {"type": "image_url", "image_url": {"url": base64_image}}
                    ]
                }
            ]
            response = client.chat.completions.create(model=model_name, messages=messages)
            content = response.choices[0].message.content.strip()
            usage = response.usage
            return content, usage.prompt_tokens, usage.completion_tokens
        except Exception as e:
            return f"[Error: {e}]", 0, 0

    @staticmethod
    def _generate_caption_bedrock(
        client: Any,
        model_name: str,
        image_path: str
    ) -> Tuple[str, int, int]:
        """
        Generate caption using AWS Bedrock style client.

        Args:
            client: Bedrock client instance.
            model_name: Model identifier.
            image_path: Path to the image.

        Returns:
            Tuple of (caption string, input tokens, output tokens).
        """
        ext = image_path.split(".")[-1].lower()
        if ext not in {"jpg", "jpeg", "png"}:
            ext = "jpeg"

        try:
            with open(image_path, "rb") as f:
                b64_bytes = base64.b64encode(f.read()).decode("utf-8")

            system_list = [{"text": "Describe this image in one sentence."}]
            message_list = [{
                "role": "user",
                "content": [
                    {"image": {"format": ext, "source": {"bytes": b64_bytes}}}
                ]
            }]
            native_request = {
                "schemaVersion": "messages-v1",
                "messages": message_list,
                "system": system_list,
            }

            response = client.invoke_model(
                modelId=model_name,
                body=json.dumps(native_request),
                contentType="application/json",
                accept="application/json"
            )

            response_body = json.loads(response["body"].read())
            output_msg = response_body["output"]["message"]["content"][0]["text"]

            usage = response_body.get("usage", {})
            input_tokens = usage.get("inputTokens", 0)
            output_tokens = usage.get("outputTokens", 0)

            return output_msg.strip(), input_tokens, output_tokens
        except Exception as e:
            return f"[Bedrock Error: {e}]", 0, 0

    @staticmethod
    def encode_image_base64(image_path: str) -> str:
        """
        Read and encode an image file as a base64 string suitable for embedding in requests.

        Args:
            image_path: Path to the image file.

        Returns:
            Base64-encoded image string with data URI prefix.
        """
        with open(image_path, "rb") as f:
            encoded = base64.b64encode(f.read()).decode("utf-8")
        return f"data:image/jpeg;base64,{encoded}"
